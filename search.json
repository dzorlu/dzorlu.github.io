[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tinkerings",
    "section": "",
    "text": "My notes to build mental models for how things work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro GPU Kernels\n\n\n\n\n\n\n\n\nSep 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nContext Parallelism\n\n\n\n\n\n\n\n\nSep 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExpert Parallelism\n\n\n\n\n\n\n\n\nSep 1, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/expert_parallel.html",
    "href": "posts/expert_parallel.html",
    "title": "Expert Parallelism",
    "section": "",
    "text": "Let’s distill how to run a Mixture-of-Experts (MoE) model with expert parallelism with an example.\nThe setup is 8 GPUs using a 2-D device mesh with Expert Parallel (EP) and FSDP2 (fully_shard).\nThis short post explains who stores which weights, where all-to-all happens, and includes minimal code (meshes and wrapping)."
  },
  {
    "objectID": "posts/expert_parallel.html#tldr",
    "href": "posts/expert_parallel.html#tldr",
    "title": "Expert Parallelism",
    "section": "TL;DR",
    "text": "TL;DR\n\nColumns move tokens. Rows share weights.\nCols (dp_shard_in_ep) = EP axis → which experts live where + token all-to-all.\nRows (dp_shard_mod_ep) = FSDP axis → how owned experts are sharded.\nNon-MoE: FSDP(8) across all ranks (no persistent replication).\nExperts: EP(2) across columns (ownership) × FSDP(4) across rows (shards).\nEP all-to-all is row-local pairs: (0↔︎1), (2↔︎3), (4↔︎5), (6↔︎7). Microbatches can differ (variable-size A2A handles it)."
  },
  {
    "objectID": "posts/expert_parallel.html#device-mesh-rank-layout",
    "href": "posts/expert_parallel.html#device-mesh-rank-layout",
    "title": "Expert Parallelism",
    "section": "Device Mesh & Rank Layout",
    "text": "Device Mesh & Rank Layout\nArrange 8 GPUs as a 4×2 grid:\n(rows, cols) → rank\n(0,0)→0   (0,1)→1\n(1,0)→2   (1,1)→3\n(2,0)→4   (2,1)→5\n(3,0)→6   (3,1)→7\n\nRows (dp_shard_mod_ep) size 4 → FSDP sharding axis for experts.\nCols (dp_shard_in_ep) size 2 → EP ownership + row-local all-to-all.\nFlattened dp view (rows×cols) size 8 → dataloader + FSDP for non-MoE.\n\nHere is the pytorch code.\ndp_shard_in_ep = ep_size                # borrowed by EP (forms EP groups)\ndp_shard_mod_ep = dp_shard // ep_size   # leftover for FSDP sharding. borrowing!\n\n# Create the 2D mesh\ndevice_mesh = init_device_mesh(\n    \"cuda\",\n    (dp_shard_mod_ep, dp_shard_in_ep),\n    mesh_dim_names=(\"dp_shard\", \"ep\"),\n)\n\n# Create aliases for DP\n# DP will be used for data loading\ndevice_mesh[(\"dp_shard\", \"ep\")]._flatten(mesh_dim_name=\"dp\")\n\ndp_mesh  = device_mesh[\"dp\"]     # size = R*C = 8  -&gt; used for non-MoE FSDP\nrow_mesh = device_mesh[\"dp_shard\"]   # size = R   = 4  -&gt; used for expert FSDP (inside each column)\ncol_mesh = device_mesh[\"ep\"]   # size = C   = 2  -&gt; used for EP ownership + a2a"
  },
  {
    "objectID": "posts/expert_parallel.html#who-stores-which-expert",
    "href": "posts/expert_parallel.html#who-stores-which-expert",
    "title": "Expert Parallelism",
    "section": "Who Stores Which Expert?",
    "text": "Who Stores Which Expert?\nAssume 8 experts E0..E7. With EP=2:\n\nColumn 0 owns experts E0–E3 (no copy of E4–E7).\nColumn 1 owns experts E4–E7 (no copy of E0–E3).\nInside each column, FSDP(4) shards the owned experts across the 4 rows.\nThis makes expert-to-expert communication possible. Rank 0 has E0–E3 and Rank 1 has E4-E7.\n\n\n\n\nRank\nColumn owns\nThis rank holds (expert weights)\n\n\n\n\n0 = (0,0)\nE0–E3\n1/4 shard of E0–E3\n\n\n2 = (1,0)\nE0–E3\n1/4 shard of E0–E3\n\n\n4 = (2,0)\nE0–E3\n1/4 shard of E0–E3\n\n\n6 = (3,0)\nE0–E3\n1/4 shard of E0–E3\n\n\n1 = (0,1)\nE4–E7\n1/4 shard of E4–E7\n\n\n3 = (1,1)\nE4–E7\n1/4 shard of E4–E7\n\n\n5 = (2,1)\nE4–E7\n1/4 shard of E4–E7\n\n\n7 = (3,1)\nE4–E7\n1/4 shard of E4–E7\n\n\n\nNon-MoE (embeddings, attention, MLP, norms) are FSDP-sharded 8-way across all ranks (flattened dp). There is no persistent replication."
  },
  {
    "objectID": "posts/expert_parallel.html#where-does-all-to-all-happen",
    "href": "posts/expert_parallel.html#where-does-all-to-all-happen",
    "title": "Expert Parallelism",
    "section": "Where Does All-to-All Happen?",
    "text": "Where Does All-to-All Happen?\nEP all-to-all is row-local:\n\nRow 0: GPU0 ⇄ GPU1\nRow 1: GPU2 ⇄ GPU3\nRow 2: GPU4 ⇄ GPU5\nRow 3: GPU6 ⇄ GPU7\n\nEach rank runs the router on its own microbatch. Tokens are split by destination column (owner of the chosen expert), then a size-exchange + variable-size all-to-all exchanges exactly those slices between the two columns in the same row.\nAfter dispatch, each rank holds only tokens for experts it owns. Expert matmuls then use FSDP across the 4 rows in that column (all-gather → compute → reduce-scatter). Finally, the inverse all-to-all returns outputs to the source ranks.\nLet’s illustrate the token dispatch operation with a well-annotated code snippet.\ndef _token_dispatch(self, model, inputs, device_mesh):\n    \"\"\"\n        All-to-all communication\n        input_splits is different coming from each device (assuming some data parallelism)\n    \"\"\"\n    ep_size = device_mesh.shape[0]\n    x_gathered, num_tokens_per_expert = inputs\n    num_tokens_per_expert_group = num_tokens_per_expert.new_empty(\n        num_tokens_per_expert.shape[0]\n    )\n\n    # distributed transpose operation.\n    # 0th GPU gets all 0th row\n\n    # Preliminary all-to-all to exchange token counts. This is used to\n    # calculate the split sizes for the main token all-to-all dispatch.\n    #\n    # Before (on GPU 0):\n    #   `num_tokens_per_expert`: [10, 5, 12, 8, 11, 6, 13, 7]\n    #   (Counts of local tokens for all 8 global experts)\n    #\n    # After (on GPU 0, which hosts experts 0 and 1):\n    #   `num_tokens_per_expert_group` is filled with:\n    #   [10, 5, | 9, 4, | 14, 2, | 3, 11]\n    #   (Counts for my local experts [E0,E1] from GPU0, GPU1, GPU2, GPU3)\n    \n    dist.all_to_all_single(\n        num_tokens_per_expert_group, # output!\n        num_tokens_per_expert, # input\n        group=device_mesh.get_group(),\n    )\n\n\n    input_splits = num_tokens_per_expert.view(\n        ep_size, -1\n    ).sum(dim=1).to(torch.device(\"cpu\"))\n\n    output_splits = num_tokens_per_expert_group.view(\n        ep_size, -1\n    ).sum(dim=1).to(torch.device(\"cpu\"))\n\n    self.input_splits = input_splits.tolist()\n    self.output_splits = output_splits.tolist()\n\n    # this is an uneven communication (e.g. ragged), where each GPU receives an uneven amount of tokens.\n\n    # On GPU 0:\n    # - Total tokens before send (sum of num_tokens_per_expert): 72\n    # - input_splits (how to slice the 72 tokens for sending): [15, 20, 17, 20]\n    # - output_splits (how many tokens to expect from each GPU): [15, 13, 16, 14]\n\n    # Before all_to_all, each GPU has a different number of tokens and a different plan:\n    # GPU 0: tensor of size 72, sends chunks of [15, 20, 17, 20]\n    # GPU 1: (example) tensor of size 80, sends chunks of [13, 25, 22, 20]\n    # GPU 2: (example) tensor of size 75, sends chunks of [16, 18, 21, 20]\n    # GPU 3: (example) tensor of size 68, sends chunks of [14, 15, 19, 20]\n\n    # After all_to_all on GPU 0:\n    # - Receives: 15 from GPU0, 13 from GPU1, 16 from GPU2, 14 from GPU3\n    # - Output tensor size = sum(output_splits) = 15 + 13 + 16 + 14 = 58\n    # - This new tensor of 58 tokens contains data for GPU 0's local experts (E0, E1),\n    #   but is grouped by source GPU, not by expert ID. It needs a local shuffle.\n\n    # all_to_all_single_autograd allows differentiable data transfer\n    print(f\"{self.output_splits=} {self.input_splits=}\")\n\n    x_gathered = all_to_all_single_autograd(\n        x_gathered,\n        self.output_splits,\n        self.input_splits,\n        device_mesh.get_group(),\n    )\n\n    # num_tokens_per_expert_group\n    #   [10, 5, | 9, 4, | 14, 2, | 3, 11]\n    # \n    #   x_gathered on GPU 0 (shape: [58, h])\n    #  +------------------------------------------------+\n    #  |                                                |\n    #  |  Block of 15 tokens RECEIVED from GPU 0        |\n    #  |  (Contains 10 tokens for MY E0, 5 for MY E1)   |\n    #  |                                                |\n    #  +------------------------------------------------+  &lt;-- Boundary at index 14\n    #  |                                                |\n    #  |  Block of 13 tokens RECEIVED from GPU 1        |\n    #  |  (Contains 9 tokens for MY E0, 4 for MY E1)    |\n    #  |                                                |\n    #  +------------------------------------------------+  &lt;-- Boundary at index 27 (14+13)\n    #  |                                                |\n    #  |  Block of 16 tokens RECEIVED from GPU 2        |\n    #  |  (Contains 14 tokens for MY E0, 2 for MY E1)   |\n    #  |                                                |\n    #  +------------------------------------------------+  &lt;-- Boundary at index 43 (27+16)\n    #  |                                                |\n    #  |  Block of 14 tokens RECEIVED from GPU 3        |\n    #  |  (Contains 3 tokens for MY E0, 11 for MY E1)   |\n    #  |                                                |\n    #  +------------------------------------------------+  &lt;-- Final boundary at index 57\n\n    #   Target layout for x_gathered (shape: [58, h])\n    #  +------------------------------------------------+\n    #  |                                                |\n    #  |  All 36 tokens for MY Expert 0                 |\n    #  |  (Gathered from the 4 blocks above)            |\n    #  |                                                |\n    #  +------------------------------------------------+  &lt;-- Boundary at index 35\n    #  |                                                |\n    #  |  All 22 tokens for MY Expert 1                 |\n    #  |  (Gathered from the 4 blocks above)            |\n    #  |                                                |\n    #  +------------------------------------------------+ \n\n    # target for num_tokens_per_expert_group\n    #    [36, 22]\n\n\n    # Reshape to see GPU-expert structure\n    tokens = num_tokens_per_expert_group.view(-1, ep_size)  \n    # Shape: [4, 2] where dim0=GPU, dim1=expert\n    # [[10,  5],  &lt;- GPU 0: 10 tokens for E0, 5 for E1\n    #  [ 9,  4],  &lt;- GPU 1: 9 tokens for E0, 4 for E1\n    #  [14,  2],  &lt;- GPU 2: 14 tokens for E0, 2 for E1\n    #  [ 3, 11]]  &lt;- GPU 3: 3 tokens for E0, 11 for E1\n    expert_per_device = num_tokens_per_expert_group.shape[0] // ep_size\n    expert_ids = torch.repeat_interleave(\n        torch.arange(expert_per_device).repeat(ep_size).to('cuda'),  # [0, 1, 0, 1, 0, 1, 0, 1] - expert pattern\n        num_tokens_per_expert_group  # [10,5,9,4,14,2,3,11] - repeat counts\n    )\n    \n    # index looks like\n    # tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 15, 16, 17, 18, 19, 20, 21, 22,\n    # 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 46,\n    # 10, 11, 12, 13, 14, 24, 25, 26, 27, 42, 43, 47, 48, 49, 50, 51, 52, 53,\n    # 54, 55, 56, 57])\n    self.index = torch.argsort(expert_ids, stable=True)\n    x_reorganized = x_gathered[self.index, :]\n\n    # per expert aggregation\n    num_tokens_per_expert_group_agg = tokens.sum(dim=1)\n\n    return x_reorganized, num_tokens_per_expert_group_agg"
  },
  {
    "objectID": "posts/expert_parallel.html#the-three-boxes-in-the-forward",
    "href": "posts/expert_parallel.html#the-three-boxes-in-the-forward",
    "title": "Expert Parallelism",
    "section": "The Three “Boxes” in the Forward",
    "text": "The Three “Boxes” in the Forward\n\nNon-experts: FSDP(8) across all ranks\nall_gather(8) → compute → reduce_scatter(8)\nEP dispatch/return: row-local all-to-all\nPairs: (0↔1), (2↔3), (4↔5), (6↔7). Size-exchange then A2A.\nExperts: FSDP(4) inside each column\nall_gather(4) → grouped GEMM → reduce_scatter(4)\n\nFor visually initiated, here is how the placement looks:\n🎮 View Interactive 3D Visualization - See how data flows through the Non-MoE, MoE FSDP, and Token Routing layers."
  },
  {
    "objectID": "posts/expert_parallel.html#minimal-mesh-wrapping",
    "href": "posts/expert_parallel.html#minimal-mesh-wrapping",
    "title": "Expert Parallelism",
    "section": "Minimal Mesh & Wrapping",
    "text": "Minimal Mesh & Wrapping\n\n# 0) EP: assign experts to columns + install dispatch/combine hooks\nclass ExpertParallel(ParallelStyle):\n    def __init__(self):\n        super().__init__()\n        self.input_splits = None\n        self.output_splits = None\n\n    # performing all-to-all dispatch on the input\n    def _token_dispatch(self, mod, inputs, device_mesh):\n        ...\n\n    @staticmethod\n    def _partition_fn(name, mod, device_mesh):\n        # shard on the expert dimension\n        for name, param in mod.named_parameters(recurse=False):\n            dist_param = nn.Parameter(distribute_tensor(param, device_mesh, [Shard(0)]))\n            mod.register_parameter(name, dist_param)\n\n    # performing all-to-all combine on the output\n    def _token_combine(self, mod, routed_output, device_mesh):\n        routed_output = all_to_all_single_autograd(\n            routed_output,\n            self.input_splits,\n            self.output_splits,\n            device_mesh.get_group(),\n        )\n        return routed_output\n\n    def _apply(self, module: nn.Module, device_mesh: DeviceMesh) -&gt; nn.Module:\n        return distribute_module(\n            module,\n            device_mesh,\n            partition_fn=ExpertParallel._partition_fn,\n            input_fn=self._token_dispatch,\n            output_fn=self._token_combine,\n        )\n\n\nparallelize_module(\n    model, device_mesh=col_mesh,\n    parallelize_plan={\"layers.*.moe.experts\": ExpertParallel()},\n)\n\n# 1) FSDP on experts (rows): shard column-local experts across rows\nfor tb in model.transformer_blocks:\n    fully_shard(tb.moe.experts, mesh=row_mesh, reshard_after_forward=False)\n\n# 2) FSDP on each block (dp): shard non-MoE 8-way\nfor tb in model.transformer_blocks:\n    fully_shard(tb, mesh=dp_mesh, reshard_after_forward=False)   # experts already DTensors on rows\n\n# 3) FSDP on root (dp): embeddings / head / leftovers\nfully_shard(model, mesh=dp_mesh, reshard_after_forward=True)     # True/False per memory tradeoff"
  },
  {
    "objectID": "posts/expert_parallel.html#pseudo-forward",
    "href": "posts/expert_parallel.html#pseudo-forward",
    "title": "Expert Parallelism",
    "section": "Pseudo-Forward",
    "text": "Pseudo-Forward\ndef forward_on_rank(row, col, x_rc):\n    # A) Non-MoE on FSDP: AG(8) → compute → RS(8)\n    h = non_moE_stack(x_rc)\n\n    # Router: expert id per token (e.g., [2,2,5,1,6,...])\n    eids = router(h)\n\n    # B) EP dispatch within this row: send tokens to owner column\n    h_owned = ep_a2a_dispatch(h, eids, group={(row,0),(row,1)})\n\n    # C) Experts on owner column: FSDP(4) across rows of this column\n    y_local = experts_matmul(h_owned)           # AG(4) → GEMM → RS(4)\n\n    # D) EP combine within this row: inverse A2A back to source rank\n    y = ep_a2a_combine(y_local, eids, group={(row,0),(row,1)})\n\n    # E) Tail Non-MoE on dp: AG(8) → compute → RS(8)\n    out = non_moE_tail(y)\n    return out\nBackward (intuition): EP’s A2A autograd returns activation grads to sources; expert parameter grads are reduce-scattered across the row group (the 4 that shard that expert). Non-MoE grads reduce-scatter across all 8."
  },
  {
    "objectID": "posts/expert_parallel.html#after-parallelization-expected-placements",
    "href": "posts/expert_parallel.html#after-parallelization-expected-placements",
    "title": "Expert Parallelism",
    "section": "After Parallelization: Expected Placements",
    "text": "After Parallelization: Expected Placements\ndef show_placements(model, keys=(\"moe.experts\",\"attention\",\"router\",\"embeddings\",\"norm\",\"output\")):\n    for name, p in model.named_parameters():\n        if any(k in name for k in keys):\n            pl = getattr(p, \"placements\", None)\n            kind = \"DTensor\" if pl is not None else \"LOCAL\"\n            print(f\"{name:&lt;60} -&gt; {kind:7} {pl}\")\nExample lines you should see:\ntok_embeddings.weight                                   -&gt; DTensor (Shard(dim=0),)\nlayers.0.attention.wq.weight                            -&gt; DTensor (Shard(dim=0),)\nlayers.0.feed_forward.router.router.weight              -&gt; DTensor (Shard(dim=0),)\nlayers.0.feed_forward.experts.w1                        -&gt; DTensor (_StridedShard(dim=0, sf=2), Shard(dim=0))\nlayers.0.feed_forward.experts.w2                        -&gt; DTensor (_StridedShard(dim=0, sf=2), Shard(dim=0))\nlayers.0.feed_forward.experts.w3                        -&gt; DTensor (_StridedShard(dim=0, sf=2), Shard(dim=0))\n...\n\nLegend\n\n(Shard(dim=0),) → Non-MoE params sharded on a 1-D mesh (flattened dp → 8-way).\n(_StridedShard(dim=0, sf=2), Shard(dim=0)) → Experts are split on two separete mesh axes of the same tensor dim-0 (experts):\n\nShard(dim=0) on cols (EP ownership).\n_StridedShard(dim=0, sf=2) on rows (FSDP across 4 rows, after a prior split by 2 cols). sf=2 because dim-0 was already split once by the 2 columns"
  },
  {
    "objectID": "posts/expert_parallel.html#appendix-dataloader-on-flattened-dp.",
    "href": "posts/expert_parallel.html#appendix-dataloader-on-flattened-dp.",
    "title": "Expert Parallelism",
    "section": "Appendix: Dataloader on Flattened DP.",
    "text": "Appendix: Dataloader on Flattened DP.\n# Use the flattened 8-way dp mesh for sampling\nglobal_dp_rank = dp_mesh.get_rank()\nnum_replicas   = dp_mesh.size()\n\nfrom torch.utils.data import DistributedSampler, DataLoader\nsampler = DistributedSampler(dataset, num_replicas=num_replicas, rank=global_dp_rank)\nloader  = DataLoader(dataset, batch_size=per_rank_bsz, sampler=sampler, pin_memory=True)\nThis preserves 8 microbatches per step—one per rank—while placements and collectives follow the mesh rules above."
  },
  {
    "objectID": "posts/triton.html",
    "href": "posts/triton.html",
    "title": "Intro GPU Kernels",
    "section": "",
    "text": "The most fundamental truth about GPU programming: memory is a flat 1D array. Your tensor[M, N] doesn’t exist—it’s just multi-dimensional indexing over M*N consecutive bytes.\n# What you write\nA = torch.randn(1024, 768)  # A[i,j]\n\n# What actually exists in memory (row-major)\nmemory = [row0_col0, row0_col1, ..., row0_col767,\n          row1_col0, row1_col1, ..., row1_col767, ...]\n# A[i,j] → memory[i*768 + j]\nCoalescing and performance depend on how your multi-dimensional indexing walks that 1D memory. When 32 threads in a warp access memory, the hardware can only coalesce these into a single transaction if your indexing pattern makes them access consecutive addresses:\n# Row-major traversal: threads access A[i, 0:32]\n# Memory walk: [i*768+0, i*768+1, ..., i*768+31]\n# Consecutive addresses → 1 coalesced transaction\n\n# Column-major traversal: threads access A[0:32, j]  \n# Memory walk: [0*768+j, 1*768+j, ..., 31*768+j]\n# Addresses 768 elements apart → 32 separate transactions\nThis is why choosing the right stride matters:\nx = torch.randn(4096, 4096, device='cuda')\n\n# Row-wise sum: indexing walks memory sequentially\nx.sum(dim=1)  # Fast - stride of 1 through memory\n\n# After transpose: logical rows are physical columns\ny = x.T  # No data movement, just changes stride from (4096, 1) to (1, 4096)\ny.sum(dim=1)  # ~30x slower - stride of 4096 through memory\n\n# .contiguous() reorganizes memory to match logical view\nz = y.contiguous()  # Copies data so logical indexing = sequential walk\nz.sum(dim=1)  # Fast again\n\n\n\nModern GPUs have massive compute but limited memory bandwidth. The key metric is arithmetic intensity (AI):\nAI = FLOPs / Bytes Transferred\nTo be compute-bound rather than memory-bound, you need high arithmetic intensity. On a B200: - Compute: ~40 TFLOPS (FP32)\n- Memory bandwidth: ~8 TB/s - Breakeven AI: ~5 FLOPs/byte\nConsider different operations:\n# Vector addition: y = a + b\n# Bytes: read 2*N*4, write N*4 = 12N bytes\n# FLOPs: N additions\n# AI = N / 12N = 0.083 FLOPs/byte → Memory bound!\n\n# Matrix multiply: C[M,N] = A[M,K] @ B[K,N]\n# Bytes: (M*K + K*N + M*N) * 4\n# FLOPs: 2*M*N*K (multiply + add)\n# AI = 2*M*N*K / ((M*K + K*N + M*N) * 4)\n# For M=N=K=4096: AI ≈ 683 FLOPs/byte → Compute bound!\n\n\n\nStandard attention has a fundamental problem:\n# Standard attention: O = softmax(QK^T)V\n# Must materialize S = QK^T of size [N, N]\n# Memory accesses: O(N²) reads/writes to HBM\n# Compute: O(N²) \n# AI: O(1) → Memory bound!\nFlash Attention’s key insight: tiling changes the memory access pattern.\n# Flash Attention with tile size T\n# Process attention in T×T blocks, never materializing full [N,N]\n# Memory accesses: O(N²/T) to HBM  \n# Compute: still O(N²)\n# AI: O(T) → Can be compute bound!\nThe magic is keeping tiles in the memory hierarchy: - HBM (main memory): ~8 TB/s - L2 cache: ~100 TB/s - SRAM (shared memory): ~150 TB/s\n- Registers: Even faster\nBy computing in tiles that fit in SRAM, Flash Attention: 1. Loads each KV block from HBM only once per query block instead of N times 2. Performs all computations in fast SRAM\n3. Only writes final output to HBM\nThe tiling transforms how we walk through memory—instead of repeatedly traversing the entire KV cache, we load contiguous blocks once and reuse them maximally.\n\n\n\nCUDA forces you to manually manage how threads walk through memory:\n// CUDA: You explicitly map threads to memory\n__global__ void add_kernel(float* x, float* y, int n) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;  // Your indexing\n    if (tid &lt; n) {\n        y[tid] = x[tid] + 1.0f;  // You ensure coalescing\n    }\n}\n// Launch: add_kernel&lt;&lt;&lt;num_blocks, threads_per_block&gt;&gt;&gt;(x, y, n);\nTriton abstracts this—you think in vectors, it handles the memory walk:\n@triton.jit\ndef add_kernel(x_ptr, y_ptr, n, BLOCK_SIZE: tl.constexpr):\n    # Program ID - which block are we?\n    pid = tl.program_id(0)\n    \n    # This program handles elements [pid*BLOCK_SIZE : (pid+1)*BLOCK_SIZE]\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n\n    \n    # Vector operations - Triton ensures coalesced walk\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = x + 1.0\n    tl.store(y_ptr + offsets, y, mask=mask)\n\n# Launch: each program processes BLOCK_SIZE elements\ngrid = (triton.cdiv(n, BLOCK_SIZE),)\nadd_kernel[grid](x, y, n, BLOCK_SIZE=1024)\nWhat Triton abstracts: - Thread-to-memory mapping: You specify vector operations, compiler assigns optimal thread walks - Coalescing patterns: tl.load/store automatically generates coalesced access patterns - Shared memory management: Compiler handles SRAM allocation and synchronization - Register allocation: Automatic register reuse for intermediates\nThe notation kernel[grid](args) launches the kernel on a grid of programs (blocks), where each program operates on vectors of data.\n\n\n\nHere’s how tiled matmul achieves high arithmetic intensity through careful memory traversal.\n\n\nFor matrix multiplication C = A @ B where: - A has shape [M, K] - B has shape [K, N]\n- C has shape [M, N]\nWe launch (M // BLOCK_M) * (N // BLOCK_N) kernels. Each kernel computes one tile of C with shape [BLOCK_M, BLOCK_N] by: - Loading blocks from one row-strip of A - Loading blocks from one column-strip of B - Accumulating partial products\n\n\n\nThe L2 cache optimization happens at the kernel scheduling level—each kernel computes one block, and GROUP_SIZE_M controls the order in which kernels execute to maximize L2 reuse between kernels.\nHow GROUP_SIZE_M=2 Reorganizes Execution:\nStandard Row-Major Order         Grouped Column-Major Order\n(process entire row 0 first)     (process 2 rows at a time)\n\nBlock IDs:                       Execution Order:\n[0]  [1]  [2]  [3]  [4]  [5]    [0]  [2]  [4]  [6]  [8]  [10]\n[6]  [7]  [8]  [9]  [10] [11]   [1]  [3]  [5]  [7]  [9]  [11]\n[12] [13] [14] [15] [16] [17] → [12] [14] [16] [18] [20] [22]\n[18] [19] [20] [21] [22] [23]   [13] [15] [17] [19] [21] [23]\n[24] [25] [26] [27] [28] [29]   [24] [26] [28] [30] [32] [34]\n[30] [31] [32] [33] [34] [35]   [25] [27] [29] [31] [33] [35]\nWhy this matters for cache:\nStandard row-major:              Grouped (GROUP_SIZE_M=2):\nBlock 0: A[row0] × B[col0]      Block 0: A[row0] × B[col0]\nBlock 1: A[row0] × B[col1]      Block 1: A[row1] × B[col0] ← B[col0] reused!\nBlock 2: A[row0] × B[col2]      Block 2: A[row0] × B[col1]\nBlock 3: A[row0] × B[col3]      Block 3: A[row1] × B[col1] ← B[col1] reused!\nBlock 4: A[row0] × B[col4]      ...\nBlock 5: A[row0] × B[col5]      \nBlock 6: A[row1] × B[col0]      \n         ↑ B[col0] likely evicted!\nGROUP_SIZE_M=2 keeps both A[row0] and A[row1] in L2 cache across kernel launches. Result: 10-15% performance improvement.\n\n\n\nWith grouped column-major ordering, we remap pid to get better cache reuse:\nExample: pid=9, M=768, N=768, BLOCK_M=128, BLOCK_N=128, GROUP_SIZE_M=2\n\nnum_pid_m = 768/128 = 6 rows of blocks\nnum_pid_n = 768/128 = 6 cols of blocks  \nnum_pid_in_group = 2 * 6 = 12 (process 2 rows × 6 cols before moving on)\n\ngroup_id = 9 // 12 = 0 (still in first group)\nfirst_pid_m = 0 * 2 = 0 (first group starts at row 0)\n\npid_m = 0 + ((9 % 12) % 2) = 0 + (9 % 2) = 1\npid_n = (9 % 12) // 2 = 9 // 2 = 4\n\nSo pid=9 computes C[128:256, 512:640]\n          (row block 1, col block 4)\n\n\n\nTo compute C[128:256, 512:640], we need: - From A: A[128:256, 0:K] (128 rows, all K columns) - From B: B[0:K, 512:640] (all K rows, 128 columns)\nBut K=4096 won’t fit in SRAM! We process in chunks:\nFull matrix multiply:\nC[128:256, 512:640] = A[128:256, 0:4096] @ B[0:4096, 512:640]\n\nSplit into K chunks:\nIteration 0: A[128:256, 0:64]    @ B[0:64, 512:640]    → accumulate\nIteration 1: A[128:256, 64:128]  @ B[64:128, 512:640]  → accumulate  \nIteration 2: A[128:256, 128:192] @ B[128:192, 512:640] → accumulate\n... (64 iterations total for K=4096, BLOCK_K=64)\nHow strides create the pointer grid:\nFor A stored row-major with shape [M, K]:\nMemory is 1D: A[i,j] maps to memory[i*K + j]\n\nExample: Loading A[128:256, 192:256] where K=4096\n\nRow 128, cols 192-255: memory[128*4096 + 192] to memory[128*4096 + 255]\n                        = positions 524480 to 524543 (contiguous!)\n                        \nRow 129, cols 192-255: memory[129*4096 + 192] to memory[129*4096 + 255]  \n                        = positions 528576 to 528639 (contiguous!)\n                        \nThe pattern:\n- Each row is contiguous in memory (64 consecutive elements)\n- Rows are K=4096 elements apart in memory\n- stride_am=4096 (jump to next row), stride_ak=1 (next column)\nEach K iteration loads tiles that fit in SRAM (128×64 = 8KB), accumulates the partial product, then moves to the next K chunk. The grouped ordering ensures A blocks stay hot in L2 across multiple B column blocks.\nThe key insight: by controlling how our multi-dimensional indexing walks through 1D memory—using tiles, grouped execution, and well-chosen strides—we transform memory-bound operations into compute-bound ones."
  },
  {
    "objectID": "posts/triton.html#memory-is-always-1d",
    "href": "posts/triton.html#memory-is-always-1d",
    "title": "Intro GPU Kernels",
    "section": "",
    "text": "The most fundamental truth about GPU programming: memory is a flat 1D array. Your tensor[M, N] doesn’t exist—it’s just multi-dimensional indexing over M*N consecutive bytes.\n# What you write\nA = torch.randn(1024, 768)  # A[i,j]\n\n# What actually exists in memory (row-major)\nmemory = [row0_col0, row0_col1, ..., row0_col767,\n          row1_col0, row1_col1, ..., row1_col767, ...]\n# A[i,j] → memory[i*768 + j]\nCoalescing and performance depend on how your multi-dimensional indexing walks that 1D memory. When 32 threads in a warp access memory, the hardware can only coalesce these into a single transaction if your indexing pattern makes them access consecutive addresses:\n# Row-major traversal: threads access A[i, 0:32]\n# Memory walk: [i*768+0, i*768+1, ..., i*768+31]\n# Consecutive addresses → 1 coalesced transaction\n\n# Column-major traversal: threads access A[0:32, j]  \n# Memory walk: [0*768+j, 1*768+j, ..., 31*768+j]\n# Addresses 768 elements apart → 32 separate transactions\nThis is why choosing the right stride matters:\nx = torch.randn(4096, 4096, device='cuda')\n\n# Row-wise sum: indexing walks memory sequentially\nx.sum(dim=1)  # Fast - stride of 1 through memory\n\n# After transpose: logical rows are physical columns\ny = x.T  # No data movement, just changes stride from (4096, 1) to (1, 4096)\ny.sum(dim=1)  # ~30x slower - stride of 4096 through memory\n\n# .contiguous() reorganizes memory to match logical view\nz = y.contiguous()  # Copies data so logical indexing = sequential walk\nz.sum(dim=1)  # Fast again"
  },
  {
    "objectID": "posts/triton.html#the-bandwidth-wall",
    "href": "posts/triton.html#the-bandwidth-wall",
    "title": "Intro GPU Kernels",
    "section": "",
    "text": "Modern GPUs have massive compute but limited memory bandwidth. The key metric is arithmetic intensity (AI):\nAI = FLOPs / Bytes Transferred\nTo be compute-bound rather than memory-bound, you need high arithmetic intensity. On a B200: - Compute: ~40 TFLOPS (FP32)\n- Memory bandwidth: ~8 TB/s - Breakeven AI: ~5 FLOPs/byte\nConsider different operations:\n# Vector addition: y = a + b\n# Bytes: read 2*N*4, write N*4 = 12N bytes\n# FLOPs: N additions\n# AI = N / 12N = 0.083 FLOPs/byte → Memory bound!\n\n# Matrix multiply: C[M,N] = A[M,K] @ B[K,N]\n# Bytes: (M*K + K*N + M*N) * 4\n# FLOPs: 2*M*N*K (multiply + add)\n# AI = 2*M*N*K / ((M*K + K*N + M*N) * 4)\n# For M=N=K=4096: AI ≈ 683 FLOPs/byte → Compute bound!"
  },
  {
    "objectID": "posts/triton.html#breaking-the-bandwidth-wall-flash-attention",
    "href": "posts/triton.html#breaking-the-bandwidth-wall-flash-attention",
    "title": "Intro GPU Kernels",
    "section": "",
    "text": "Standard attention has a fundamental problem:\n# Standard attention: O = softmax(QK^T)V\n# Must materialize S = QK^T of size [N, N]\n# Memory accesses: O(N²) reads/writes to HBM\n# Compute: O(N²) \n# AI: O(1) → Memory bound!\nFlash Attention’s key insight: tiling changes the memory access pattern.\n# Flash Attention with tile size T\n# Process attention in T×T blocks, never materializing full [N,N]\n# Memory accesses: O(N²/T) to HBM  \n# Compute: still O(N²)\n# AI: O(T) → Can be compute bound!\nThe magic is keeping tiles in the memory hierarchy: - HBM (main memory): ~8 TB/s - L2 cache: ~100 TB/s - SRAM (shared memory): ~150 TB/s\n- Registers: Even faster\nBy computing in tiles that fit in SRAM, Flash Attention: 1. Loads each KV block from HBM only once per query block instead of N times 2. Performs all computations in fast SRAM\n3. Only writes final output to HBM\nThe tiling transforms how we walk through memory—instead of repeatedly traversing the entire KV cache, we load contiguous blocks once and reuse them maximally."
  },
  {
    "objectID": "posts/triton.html#triton-think-in-blocks-not-threads",
    "href": "posts/triton.html#triton-think-in-blocks-not-threads",
    "title": "Intro GPU Kernels",
    "section": "",
    "text": "CUDA forces you to manually manage how threads walk through memory:\n// CUDA: You explicitly map threads to memory\n__global__ void add_kernel(float* x, float* y, int n) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;  // Your indexing\n    if (tid &lt; n) {\n        y[tid] = x[tid] + 1.0f;  // You ensure coalescing\n    }\n}\n// Launch: add_kernel&lt;&lt;&lt;num_blocks, threads_per_block&gt;&gt;&gt;(x, y, n);\nTriton abstracts this—you think in vectors, it handles the memory walk:\n@triton.jit\ndef add_kernel(x_ptr, y_ptr, n, BLOCK_SIZE: tl.constexpr):\n    # Program ID - which block are we?\n    pid = tl.program_id(0)\n    \n    # This program handles elements [pid*BLOCK_SIZE : (pid+1)*BLOCK_SIZE]\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n\n    \n    # Vector operations - Triton ensures coalesced walk\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = x + 1.0\n    tl.store(y_ptr + offsets, y, mask=mask)\n\n# Launch: each program processes BLOCK_SIZE elements\ngrid = (triton.cdiv(n, BLOCK_SIZE),)\nadd_kernel[grid](x, y, n, BLOCK_SIZE=1024)\nWhat Triton abstracts: - Thread-to-memory mapping: You specify vector operations, compiler assigns optimal thread walks - Coalescing patterns: tl.load/store automatically generates coalesced access patterns - Shared memory management: Compiler handles SRAM allocation and synchronization - Register allocation: Automatic register reuse for intermediates\nThe notation kernel[grid](args) launches the kernel on a grid of programs (blocks), where each program operates on vectors of data."
  },
  {
    "objectID": "posts/triton.html#matmul-putting-it-all-together",
    "href": "posts/triton.html#matmul-putting-it-all-together",
    "title": "Intro GPU Kernels",
    "section": "",
    "text": "Here’s how tiled matmul achieves high arithmetic intensity through careful memory traversal.\n\n\nFor matrix multiplication C = A @ B where: - A has shape [M, K] - B has shape [K, N]\n- C has shape [M, N]\nWe launch (M // BLOCK_M) * (N // BLOCK_N) kernels. Each kernel computes one tile of C with shape [BLOCK_M, BLOCK_N] by: - Loading blocks from one row-strip of A - Loading blocks from one column-strip of B - Accumulating partial products\n\n\n\nThe L2 cache optimization happens at the kernel scheduling level—each kernel computes one block, and GROUP_SIZE_M controls the order in which kernels execute to maximize L2 reuse between kernels.\nHow GROUP_SIZE_M=2 Reorganizes Execution:\nStandard Row-Major Order         Grouped Column-Major Order\n(process entire row 0 first)     (process 2 rows at a time)\n\nBlock IDs:                       Execution Order:\n[0]  [1]  [2]  [3]  [4]  [5]    [0]  [2]  [4]  [6]  [8]  [10]\n[6]  [7]  [8]  [9]  [10] [11]   [1]  [3]  [5]  [7]  [9]  [11]\n[12] [13] [14] [15] [16] [17] → [12] [14] [16] [18] [20] [22]\n[18] [19] [20] [21] [22] [23]   [13] [15] [17] [19] [21] [23]\n[24] [25] [26] [27] [28] [29]   [24] [26] [28] [30] [32] [34]\n[30] [31] [32] [33] [34] [35]   [25] [27] [29] [31] [33] [35]\nWhy this matters for cache:\nStandard row-major:              Grouped (GROUP_SIZE_M=2):\nBlock 0: A[row0] × B[col0]      Block 0: A[row0] × B[col0]\nBlock 1: A[row0] × B[col1]      Block 1: A[row1] × B[col0] ← B[col0] reused!\nBlock 2: A[row0] × B[col2]      Block 2: A[row0] × B[col1]\nBlock 3: A[row0] × B[col3]      Block 3: A[row1] × B[col1] ← B[col1] reused!\nBlock 4: A[row0] × B[col4]      ...\nBlock 5: A[row0] × B[col5]      \nBlock 6: A[row1] × B[col0]      \n         ↑ B[col0] likely evicted!\nGROUP_SIZE_M=2 keeps both A[row0] and A[row1] in L2 cache across kernel launches. Result: 10-15% performance improvement.\n\n\n\nWith grouped column-major ordering, we remap pid to get better cache reuse:\nExample: pid=9, M=768, N=768, BLOCK_M=128, BLOCK_N=128, GROUP_SIZE_M=2\n\nnum_pid_m = 768/128 = 6 rows of blocks\nnum_pid_n = 768/128 = 6 cols of blocks  \nnum_pid_in_group = 2 * 6 = 12 (process 2 rows × 6 cols before moving on)\n\ngroup_id = 9 // 12 = 0 (still in first group)\nfirst_pid_m = 0 * 2 = 0 (first group starts at row 0)\n\npid_m = 0 + ((9 % 12) % 2) = 0 + (9 % 2) = 1\npid_n = (9 % 12) // 2 = 9 // 2 = 4\n\nSo pid=9 computes C[128:256, 512:640]\n          (row block 1, col block 4)\n\n\n\nTo compute C[128:256, 512:640], we need: - From A: A[128:256, 0:K] (128 rows, all K columns) - From B: B[0:K, 512:640] (all K rows, 128 columns)\nBut K=4096 won’t fit in SRAM! We process in chunks:\nFull matrix multiply:\nC[128:256, 512:640] = A[128:256, 0:4096] @ B[0:4096, 512:640]\n\nSplit into K chunks:\nIteration 0: A[128:256, 0:64]    @ B[0:64, 512:640]    → accumulate\nIteration 1: A[128:256, 64:128]  @ B[64:128, 512:640]  → accumulate  \nIteration 2: A[128:256, 128:192] @ B[128:192, 512:640] → accumulate\n... (64 iterations total for K=4096, BLOCK_K=64)\nHow strides create the pointer grid:\nFor A stored row-major with shape [M, K]:\nMemory is 1D: A[i,j] maps to memory[i*K + j]\n\nExample: Loading A[128:256, 192:256] where K=4096\n\nRow 128, cols 192-255: memory[128*4096 + 192] to memory[128*4096 + 255]\n                        = positions 524480 to 524543 (contiguous!)\n                        \nRow 129, cols 192-255: memory[129*4096 + 192] to memory[129*4096 + 255]  \n                        = positions 528576 to 528639 (contiguous!)\n                        \nThe pattern:\n- Each row is contiguous in memory (64 consecutive elements)\n- Rows are K=4096 elements apart in memory\n- stride_am=4096 (jump to next row), stride_ak=1 (next column)\nEach K iteration loads tiles that fit in SRAM (128×64 = 8KB), accumulates the partial product, then moves to the next K chunk. The grouped ordering ensures A blocks stay hot in L2 across multiple B column blocks.\nThe key insight: by controlling how our multi-dimensional indexing walks through 1D memory—using tiles, grouped execution, and well-chosen strides—we transform memory-bound operations into compute-bound ones."
  },
  {
    "objectID": "posts/context_parallel.html",
    "href": "posts/context_parallel.html",
    "title": "Context Parallelism",
    "section": "",
    "text": "Context Parallelism (CP) is a distributed training technique that enables training of LLMs with extremely long sequences (up to 1M tokens) by sharding the sequence dimension across multiple GPUs. This post traces the implementation from device mesh setup through the actual parallelization mechanics."
  },
  {
    "objectID": "posts/context_parallel.html#device-mesh-rank-layout",
    "href": "posts/context_parallel.html#device-mesh-rank-layout",
    "title": "Context Parallelism",
    "section": "Device Mesh & Rank Layout",
    "text": "Device Mesh & Rank Layout\nThe key insight about CP is that it reduces your effective data parallelism. Why? Because the same batch of data needs to be processed by all GPUs in the CP group - they each handle a different sequence chunk of that same data. For an 8 GPU setup with CP=2, your effective DP becomes 4 - you’re processing 4 unique batches, not 8.\n\nVisualizing CP vs Pure Data Parallelism\nPure Data Parallel (8 GPUs):\nGPU 0: Batch_0 [B, seq_len, D] → Linear → Output_0\nGPU 1: Batch_1 [B, seq_len, D] → Linear → Output_1\n...\nGPU 7: Batch_7 [B, seq_len, D] → Linear → Output_7\nTotal: 8 different batches processed simultaneously\n\nWith CP=2, DP=4 (8 GPUs):\nGPU 0: Batch_0 [B, seq/2 (first half), D]  → Linear → Output_0_partial\nGPU 1: Batch_0 [B, seq/2 (second half), D] → Linear → Output_0_partial\nGPU 2: Batch_1 [B, seq/2 (first half), D]  → Linear → Output_1_partial  \nGPU 3: Batch_1 [B, seq/2 (second half), D] → Linear → Output_1_partial\n...\nTotal: Only 4 different batches processed (each split across 2 GPUs)\n\n\nThe Mesh Setup\ndevice_mesh = init_device_mesh(\n    \"cuda\",\n    (args.dp_size, args.cp_size),  # (4, 2) for 8 GPUs\n    mesh_dim_names=(\"dp_shard\", \"cp\"),\n)\n\n# Flatten to create FSDP mesh - this is critical\ndevice_mesh[(\"dp_shard\", \"cp\")]._flatten(mesh_dim_name=\"dp\")\n\ndp_mesh = device_mesh[\"dp_shard\"]   # size = 4 -&gt; for data loading\nrow_mesh = device_mesh[\"dp\"]        # size = 8 -&gt; for FSDP all-reduce\ncol_mesh = device_mesh[\"cp\"]        # size = 2 -&gt; for CP operations\nThe crucial point: FSDP still needs to operate across all 8 ranks for gradient synchronization. Even though CP splits sequences within groups of 2, the model weights need to be synchronized across all 8 GPUs. That’s why we create the flattened “dp” mesh with size 8.\nWithout FSDP operating on all ranks, you’d essentially train 4 independent model replicas (one per DP group), which would never converge properly. In other words, in CP the weights are replicated across the CP group (similar to DP). Each rank computes gradients for the same weights but using different sequence positions, hence needing gradient synchronization."
  },
  {
    "objectID": "posts/context_parallel.html#context-parallel-in-practice",
    "href": "posts/context_parallel.html#context-parallel-in-practice",
    "title": "Context Parallelism",
    "section": "Context Parallel in Practice",
    "text": "Context Parallel in Practice\nThe context_parallel context applies Ring Attention to all attention layers. freqs_cis is sharded along the position axis.\n@monitor\ndef training_step(x, y, i):\n    ctx = nullcontext()\n    if args.use_cp:\n        ctx = context_parallel(\n            col_mesh,\n            buffers=[x, y, model.freqs_cis],  # Shard these tensors\n            buffer_seq_dims=[1, 1, 0],         # Along sequence dimension\n            no_restore_buffers={x, y},         # Don't unshard after forward\n        )\n    with ctx:\n        logits = model(x)\n        # All SDPA calls now use Ring Attention"
  },
  {
    "objectID": "posts/context_parallel.html#how-context-parallel-works-under-the-hood",
    "href": "posts/context_parallel.html#how-context-parallel-works-under-the-hood",
    "title": "Context Parallelism",
    "section": "How Context Parallel Works Under the Hood",
    "text": "How Context Parallel Works Under the Hood\n\nTwo-Level Replacement\nLevel 1: Python Function Wrapper\ndistribute_function is singleton-based. Once called, ALL SDPA calls use Ring Attention:\n# Input: tensors → DTensors with Shard(seq_dim)\ndef attention_input_fn(mesh, *args, **kwargs):\n    placement = [Shard(seq_dim)]\n    # Convert all tensor args to DTensors\n    \n# Output: DTensors → local tensors  \ndef attention_output_fn(mesh, outputs):\n    return output.to_local() if isinstance(output, DTensor) else output\n\n# Replace THE function object globally\nwrapper_fn = wrapper(fn, attention_input_fn, attention_output_fn)\nsetattr(F, 'scaled_dot_product_attention', wrapper_fn)\nLevel 2: ATEN Dispatch\ncall_maps = {\n    aten._scaled_dot_product_flash_attention: \n        _scaled_dot_product_ring_flash_attention,\n    aten._scaled_dot_product_efficient_attention: \n        _scaled_dot_product_ring_efficient_attention,\n}\nRegular Flash Attention: computes on local sequence only.\nRing Flash Attention: adds KV rotation between ranks.\n# Ring Attention (simplified)\ndef ring_flash_attention(q_local, k_local, v_local):\n    output = flash_attention(q_local, k_local, v_local)\n    \n    # Rotate KV chunks from other ranks\n    for k_chunk, v_chunk in rotate_kv_chunks():\n        partial = flash_attention(q_local, k_chunk, v_chunk)\n        output = merge_attention_outputs(output, partial)  # log-sum-exp\n    \n    return output\n\n\nExecution Flow\nF.scaled_dot_product_attention(q, k, v, is_causal=True)\n    ↓\nwrapper_fn(q, k, v)  # Level 1\n    ↓ \nattention_input_fn: converts to DTensor with Shard(seq_dim)\n    ↓\nDTensor dispatcher finds Ring Attention in call_maps  # Level 2\n    ↓\n_scaled_dot_product_ring_flash_attention executes\n    ↓\nattention_output_fn: converts back to local tensor\nLevel 1: tensor conversion and orchestration\nLevel 2: swaps CUDA kernel to Ring Attention variant\n\n\nMemory"
  }
]
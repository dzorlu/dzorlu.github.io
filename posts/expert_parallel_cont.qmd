

## Why EP=2 and not EP=8?

- **EP=2 + rows=4** (this README):
  - **Memory**: experts are also sharded (rows).
  - **Comms**: A2A is 2-way (cheap).
  - **Compute**: healthy per-expert batches per device → efficient grouped GEMM.

- **EP=8 + rows=1**:
  - No rows left → cannot FSDP-shard expert weights (higher memory).
  - A2A is 8-way (heavier comms) and per-expert batches per GPU get tiny (worse efficiency).
  - You can still FSDP(8) non-MoE, but overall MoE trade-offs are usually worse.

---

## Sanity Checklist

- Non-MoE params print one placement → `(Shard(dim=0),)` (FSDP-8 on dp).
- Expert params print two placements → `(_StridedShard(0,sf=2), Shard(0))` (rows×cols).
- No `Replicate()` in placements → no persistent replication.
- EP A2A pairs (0↔1), (2↔3), (4↔5), (6↔7) — microbatches can differ (size-exchange + variable-size A2A).

---
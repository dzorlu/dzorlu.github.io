<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-09-01">

<title>Expert Parallelism – Tinkerings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-45e8e462cd760eca75dbd85b13f1b7a2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-2a27ec7538e68809fa2de2390dba7afd.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="fullcontent quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Expert Parallelism</h1>
  <div class="quarto-categories">
    <div class="quarto-category">distributed-training</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Let’s distill how to run a Mixture-of-Experts (MoE) model with expert parallelism with example.</p>
<p>The setup is 8 GPUs using a 2-D device mesh with Expert Parallel (EP) and FSDP2 (fully_shard).</p>
<p>This short post explains who stores which weights, where all-to-all happens, and includes minimal code (meshes and wrapping).</p>
<hr>
<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<ul>
<li><strong>Columns move tokens. Rows share weights.</strong></li>
<li><strong>Cols</strong> (dp_shard_in_ep) = EP axis → which experts live where + token all-to-all.</li>
<li><strong>Rows</strong> (dp_shard_mod_ep) = FSDP axis → how owned experts are sharded.</li>
<li><strong>Non-MoE</strong>: FSDP(8) across all ranks (no persistent replication).</li>
<li><strong>Experts</strong>: EP(2) across columns (ownership) × FSDP(4) across rows (shards).</li>
<li><strong>EP all-to-all</strong> is row-local pairs: (0↔︎1), (2↔︎3), (4↔︎5), (6↔︎7). Microbatches can differ (variable-size A2A handles it).</li>
</ul>
<hr>
</section>
<section id="device-mesh-rank-layout" class="level2">
<h2 class="anchored" data-anchor-id="device-mesh-rank-layout">Device Mesh &amp; Rank Layout</h2>
<p>Arrange 8 GPUs as a 4×2 grid:</p>
<pre><code>(rows, cols) → rank
(0,0)→0   (0,1)→1
(1,0)→2   (1,1)→3
(2,0)→4   (2,1)→5
(3,0)→6   (3,1)→7</code></pre>
<ul>
<li><strong>Rows</strong> (dp_shard_mod_ep) size 4 → FSDP sharding axis for experts.</li>
<li><strong>Cols</strong> (dp_shard_in_ep) size 2 → EP ownership + row-local all-to-all.</li>
<li><strong>Flattened dp</strong> view (rows×cols) size 8 → dataloader + FSDP for non-MoE.</li>
</ul>
<p>Here is the pytorch code.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>dp_shard_in_ep <span class="op">=</span> ep_size                <span class="co"># borrowed by EP (forms EP groups)</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>dp_shard_mod_ep <span class="op">=</span> dp_shard <span class="op">//</span> ep_size   <span class="co"># leftover for FSDP sharding. borrowing!</span></span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="co"># Create the 2D mesh</span></span>
<span id="cb2-5"><a href="#cb2-5"></a>device_mesh <span class="op">=</span> init_device_mesh(</span>
<span id="cb2-6"><a href="#cb2-6"></a>    <span class="st">"cuda"</span>,</span>
<span id="cb2-7"><a href="#cb2-7"></a>    (dp_shard_mod_ep, dp_shard_in_ep),</span>
<span id="cb2-8"><a href="#cb2-8"></a>    mesh_dim_names<span class="op">=</span>(<span class="st">"dp_shard"</span>, <span class="st">"ep"</span>),</span>
<span id="cb2-9"><a href="#cb2-9"></a>)</span>
<span id="cb2-10"><a href="#cb2-10"></a></span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="co"># Create aliases for DP</span></span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="co"># DP will be used for data loading</span></span>
<span id="cb2-13"><a href="#cb2-13"></a>device_mesh[(<span class="st">"dp_shard"</span>, <span class="st">"ep"</span>)]._flatten(mesh_dim_name<span class="op">=</span><span class="st">"dp"</span>)</span>
<span id="cb2-14"><a href="#cb2-14"></a></span>
<span id="cb2-15"><a href="#cb2-15"></a>dp_mesh  <span class="op">=</span> device_mesh[<span class="st">"dp"</span>]     <span class="co"># size = R*C = 8  -&gt; used for non-MoE FSDP</span></span>
<span id="cb2-16"><a href="#cb2-16"></a>row_mesh <span class="op">=</span> device_mesh[<span class="st">"dp_shard"</span>]   <span class="co"># size = R   = 4  -&gt; used for expert FSDP (inside each column)</span></span>
<span id="cb2-17"><a href="#cb2-17"></a>col_mesh <span class="op">=</span> device_mesh[<span class="st">"ep"</span>]   <span class="co"># size = C   = 2  -&gt; used for EP ownership + a2a</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="who-stores-which-expert" class="level2">
<h2 class="anchored" data-anchor-id="who-stores-which-expert">Who Stores Which Expert?</h2>
<p>Assume 8 experts <code>E0..E7</code>. With EP=2: - <strong>Column 0</strong> owns experts E0–E3 (no copy of E4–E7). - <strong>Column 1</strong> owns experts E4–E7 (no copy of E0–E3). - Inside each column, FSDP(4) shards the owned experts across the 4 rows. - This makes expert-to-expert communication possible. <code>Rank 0</code> has <code>E0–E3</code> and <code>Rank 1</code> has <code>E4-E7</code>.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Rank</th>
<th>Column owns</th>
<th>This rank holds (expert weights)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0 = (0,0)</td>
<td>E0–E3</td>
<td>1/4 shard of E0–E3</td>
</tr>
<tr class="even">
<td>2 = (1,0)</td>
<td>E0–E3</td>
<td>1/4 shard of E0–E3</td>
</tr>
<tr class="odd">
<td>4 = (2,0)</td>
<td>E0–E3</td>
<td>1/4 shard of E0–E3</td>
</tr>
<tr class="even">
<td>6 = (3,0)</td>
<td>E0–E3</td>
<td>1/4 shard of E0–E3</td>
</tr>
<tr class="odd">
<td>1 = (0,1)</td>
<td>E4–E7</td>
<td>1/4 shard of E4–E7</td>
</tr>
<tr class="even">
<td>3 = (1,1)</td>
<td>E4–E7</td>
<td>1/4 shard of E4–E7</td>
</tr>
<tr class="odd">
<td>5 = (2,1)</td>
<td>E4–E7</td>
<td>1/4 shard of E4–E7</td>
</tr>
<tr class="even">
<td>7 = (3,1)</td>
<td>E4–E7</td>
<td>1/4 shard of E4–E7</td>
</tr>
</tbody>
</table>
<p>Non-MoE (embeddings, attention, MLP, norms) are FSDP-sharded 8-way across all ranks (flattened dp). There is no persistent replication.</p>
<hr>
</section>
<section id="where-does-all-to-all-happen" class="level2">
<h2 class="anchored" data-anchor-id="where-does-all-to-all-happen">Where Does All-to-All Happen?</h2>
<p>EP all-to-all is <strong>row-local</strong>:</p>
<ul>
<li>Row 0: GPU0 ⇄ GPU1</li>
<li>Row 1: GPU2 ⇄ GPU3</li>
<li>Row 2: GPU4 ⇄ GPU5</li>
<li>Row 3: GPU6 ⇄ GPU7</li>
</ul>
<p>Each rank runs the router on its own microbatch. Tokens are split by destination column (owner of the chosen expert), then a size-exchange + variable-size all-to-all exchanges exactly those slices between the two columns in the same row.</p>
<p>After dispatch, each rank holds only tokens for experts it owns. Expert matmuls then use FSDP across the 4 rows in that column (<code>all-gather</code> → <code>compute</code> → <code>reduce-scatter</code>). Finally, the inverse all-to-all returns outputs to the source ranks.</p>
<p>Let’s illustrate the token dispatch operation with a well-annotated code snippet.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">def</span> _token_dispatch(<span class="va">self</span>, model, inputs, device_mesh):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="co">"""</span></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="co">        All-to-all communication</span></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="co">        input_splits is different coming from each device (assuming some data parallelism)</span></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="co">    """</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>    ep_size <span class="op">=</span> device_mesh.shape[<span class="dv">0</span>]</span>
<span id="cb3-7"><a href="#cb3-7"></a>    x_gathered, num_tokens_per_expert <span class="op">=</span> inputs</span>
<span id="cb3-8"><a href="#cb3-8"></a>    num_tokens_per_expert_group <span class="op">=</span> num_tokens_per_expert.new_empty(</span>
<span id="cb3-9"><a href="#cb3-9"></a>        num_tokens_per_expert.shape[<span class="dv">0</span>]</span>
<span id="cb3-10"><a href="#cb3-10"></a>    )</span>
<span id="cb3-11"><a href="#cb3-11"></a></span>
<span id="cb3-12"><a href="#cb3-12"></a>    <span class="co"># distributed transpose operation.</span></span>
<span id="cb3-13"><a href="#cb3-13"></a>    <span class="co"># 0th GPU gets all 0th row</span></span>
<span id="cb3-14"><a href="#cb3-14"></a></span>
<span id="cb3-15"><a href="#cb3-15"></a>    <span class="co"># Preliminary all-to-all to exchange token counts. This is used to</span></span>
<span id="cb3-16"><a href="#cb3-16"></a>    <span class="co"># calculate the split sizes for the main token all-to-all dispatch.</span></span>
<span id="cb3-17"><a href="#cb3-17"></a>    <span class="co">#</span></span>
<span id="cb3-18"><a href="#cb3-18"></a>    <span class="co"># Before (on GPU 0):</span></span>
<span id="cb3-19"><a href="#cb3-19"></a>    <span class="co">#   `num_tokens_per_expert`: [10, 5, 12, 8, 11, 6, 13, 7]</span></span>
<span id="cb3-20"><a href="#cb3-20"></a>    <span class="co">#   (Counts of local tokens for all 8 global experts)</span></span>
<span id="cb3-21"><a href="#cb3-21"></a>    <span class="co">#</span></span>
<span id="cb3-22"><a href="#cb3-22"></a>    <span class="co"># After (on GPU 0, which hosts experts 0 and 1):</span></span>
<span id="cb3-23"><a href="#cb3-23"></a>    <span class="co">#   `num_tokens_per_expert_group` is filled with:</span></span>
<span id="cb3-24"><a href="#cb3-24"></a>    <span class="co">#   [10, 5, | 9, 4, | 14, 2, | 3, 11]</span></span>
<span id="cb3-25"><a href="#cb3-25"></a>    <span class="co">#   (Counts for my local experts [E0,E1] from GPU0, GPU1, GPU2, GPU3)</span></span>
<span id="cb3-26"><a href="#cb3-26"></a>    </span>
<span id="cb3-27"><a href="#cb3-27"></a>    dist.all_to_all_single(</span>
<span id="cb3-28"><a href="#cb3-28"></a>        num_tokens_per_expert_group, <span class="co"># output!</span></span>
<span id="cb3-29"><a href="#cb3-29"></a>        num_tokens_per_expert, <span class="co"># input</span></span>
<span id="cb3-30"><a href="#cb3-30"></a>        group<span class="op">=</span>device_mesh.get_group(),</span>
<span id="cb3-31"><a href="#cb3-31"></a>    )</span>
<span id="cb3-32"><a href="#cb3-32"></a></span>
<span id="cb3-33"><a href="#cb3-33"></a></span>
<span id="cb3-34"><a href="#cb3-34"></a>    input_splits <span class="op">=</span> num_tokens_per_expert.view(</span>
<span id="cb3-35"><a href="#cb3-35"></a>        ep_size, <span class="op">-</span><span class="dv">1</span></span>
<span id="cb3-36"><a href="#cb3-36"></a>    ).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>).to(torch.device(<span class="st">"cpu"</span>))</span>
<span id="cb3-37"><a href="#cb3-37"></a></span>
<span id="cb3-38"><a href="#cb3-38"></a>    output_splits <span class="op">=</span> num_tokens_per_expert_group.view(</span>
<span id="cb3-39"><a href="#cb3-39"></a>        ep_size, <span class="op">-</span><span class="dv">1</span></span>
<span id="cb3-40"><a href="#cb3-40"></a>    ).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>).to(torch.device(<span class="st">"cpu"</span>))</span>
<span id="cb3-41"><a href="#cb3-41"></a></span>
<span id="cb3-42"><a href="#cb3-42"></a>    <span class="va">self</span>.input_splits <span class="op">=</span> input_splits.tolist()</span>
<span id="cb3-43"><a href="#cb3-43"></a>    <span class="va">self</span>.output_splits <span class="op">=</span> output_splits.tolist()</span>
<span id="cb3-44"><a href="#cb3-44"></a></span>
<span id="cb3-45"><a href="#cb3-45"></a>    <span class="co"># this is an uneven communication (e.g. ragged), where each GPU receives an uneven amount of tokens.</span></span>
<span id="cb3-46"><a href="#cb3-46"></a></span>
<span id="cb3-47"><a href="#cb3-47"></a>    <span class="co"># On GPU 0:</span></span>
<span id="cb3-48"><a href="#cb3-48"></a>    <span class="co"># - Total tokens before send (sum of num_tokens_per_expert): 72</span></span>
<span id="cb3-49"><a href="#cb3-49"></a>    <span class="co"># - input_splits (how to slice the 72 tokens for sending): [15, 20, 17, 20]</span></span>
<span id="cb3-50"><a href="#cb3-50"></a>    <span class="co"># - output_splits (how many tokens to expect from each GPU): [15, 13, 16, 14]</span></span>
<span id="cb3-51"><a href="#cb3-51"></a></span>
<span id="cb3-52"><a href="#cb3-52"></a>    <span class="co"># Before all_to_all, each GPU has a different number of tokens and a different plan:</span></span>
<span id="cb3-53"><a href="#cb3-53"></a>    <span class="co"># GPU 0: tensor of size 72, sends chunks of [15, 20, 17, 20]</span></span>
<span id="cb3-54"><a href="#cb3-54"></a>    <span class="co"># GPU 1: (example) tensor of size 80, sends chunks of [13, 25, 22, 20]</span></span>
<span id="cb3-55"><a href="#cb3-55"></a>    <span class="co"># GPU 2: (example) tensor of size 75, sends chunks of [16, 18, 21, 20]</span></span>
<span id="cb3-56"><a href="#cb3-56"></a>    <span class="co"># GPU 3: (example) tensor of size 68, sends chunks of [14, 15, 19, 20]</span></span>
<span id="cb3-57"><a href="#cb3-57"></a></span>
<span id="cb3-58"><a href="#cb3-58"></a>    <span class="co"># After all_to_all on GPU 0:</span></span>
<span id="cb3-59"><a href="#cb3-59"></a>    <span class="co"># - Receives: 15 from GPU0, 13 from GPU1, 16 from GPU2, 14 from GPU3</span></span>
<span id="cb3-60"><a href="#cb3-60"></a>    <span class="co"># - Output tensor size = sum(output_splits) = 15 + 13 + 16 + 14 = 58</span></span>
<span id="cb3-61"><a href="#cb3-61"></a>    <span class="co"># - This new tensor of 58 tokens contains data for GPU 0's local experts (E0, E1),</span></span>
<span id="cb3-62"><a href="#cb3-62"></a>    <span class="co">#   but is grouped by source GPU, not by expert ID. It needs a local shuffle.</span></span>
<span id="cb3-63"><a href="#cb3-63"></a></span>
<span id="cb3-64"><a href="#cb3-64"></a>    <span class="co"># all_to_all_single_autograd allows differentiable data transfer</span></span>
<span id="cb3-65"><a href="#cb3-65"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>output_splits<span class="op">=</span><span class="sc">}</span><span class="ss"> </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>input_splits<span class="op">=</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-66"><a href="#cb3-66"></a></span>
<span id="cb3-67"><a href="#cb3-67"></a>    x_gathered <span class="op">=</span> all_to_all_single_autograd(</span>
<span id="cb3-68"><a href="#cb3-68"></a>        x_gathered,</span>
<span id="cb3-69"><a href="#cb3-69"></a>        <span class="va">self</span>.output_splits,</span>
<span id="cb3-70"><a href="#cb3-70"></a>        <span class="va">self</span>.input_splits,</span>
<span id="cb3-71"><a href="#cb3-71"></a>        device_mesh.get_group(),</span>
<span id="cb3-72"><a href="#cb3-72"></a>    )</span>
<span id="cb3-73"><a href="#cb3-73"></a></span>
<span id="cb3-74"><a href="#cb3-74"></a>    <span class="co"># num_tokens_per_expert_group</span></span>
<span id="cb3-75"><a href="#cb3-75"></a>    <span class="co">#   [10, 5, | 9, 4, | 14, 2, | 3, 11]</span></span>
<span id="cb3-76"><a href="#cb3-76"></a>    <span class="co"># </span></span>
<span id="cb3-77"><a href="#cb3-77"></a>    <span class="co">#   x_gathered on GPU 0 (shape: [58, h])</span></span>
<span id="cb3-78"><a href="#cb3-78"></a>    <span class="co">#  +------------------------------------------------+</span></span>
<span id="cb3-79"><a href="#cb3-79"></a>    <span class="co">#  |                                                |</span></span>
<span id="cb3-80"><a href="#cb3-80"></a>    <span class="co">#  |  Block of 15 tokens RECEIVED from GPU 0        |</span></span>
<span id="cb3-81"><a href="#cb3-81"></a>    <span class="co">#  |  (Contains 10 tokens for MY E0, 5 for MY E1)   |</span></span>
<span id="cb3-82"><a href="#cb3-82"></a>    <span class="co">#  |                                                |</span></span>
<span id="cb3-83"><a href="#cb3-83"></a>    <span class="co">#  +------------------------------------------------+  &lt;-- Boundary at index 14</span></span>
<span id="cb3-84"><a href="#cb3-84"></a>    <span class="co">#  |                                                |</span></span>
<span id="cb3-85"><a href="#cb3-85"></a>    <span class="co">#  |  Block of 13 tokens RECEIVED from GPU 1        |</span></span>
<span id="cb3-86"><a href="#cb3-86"></a>    <span class="co">#  |  (Contains 9 tokens for MY E0, 4 for MY E1)    |</span></span>
<span id="cb3-87"><a href="#cb3-87"></a>    <span class="co">#  |                                                |</span></span>
<span id="cb3-88"><a href="#cb3-88"></a>    <span class="co">#  +------------------------------------------------+  &lt;-- Boundary at index 27 (14+13)</span></span>
<span id="cb3-89"><a href="#cb3-89"></a>    <span class="co">#  |                                                |</span></span>
<span id="cb3-90"><a href="#cb3-90"></a>    <span class="co">#  |  Block of 16 tokens RECEIVED from GPU 2        |</span></span>
<span id="cb3-91"><a href="#cb3-91"></a>    <span class="co">#  |  (Contains 14 tokens for MY E0, 2 for MY E1)   |</span></span>
<span id="cb3-92"><a href="#cb3-92"></a>    <span class="co">#  |                                                |</span></span>
<span id="cb3-93"><a href="#cb3-93"></a>    <span class="co">#  +------------------------------------------------+  &lt;-- Boundary at index 43 (27+16)</span></span>
<span id="cb3-94"><a href="#cb3-94"></a>    <span class="co">#  |                                                |</span></span>
<span id="cb3-95"><a href="#cb3-95"></a>    <span class="co">#  |  Block of 14 tokens RECEIVED from GPU 3        |</span></span>
<span id="cb3-96"><a href="#cb3-96"></a>    <span class="co">#  |  (Contains 3 tokens for MY E0, 11 for MY E1)   |</span></span>
<span id="cb3-97"><a href="#cb3-97"></a>    <span class="co">#  |                                                |</span></span>
<span id="cb3-98"><a href="#cb3-98"></a>    <span class="co">#  +------------------------------------------------+  &lt;-- Final boundary at index 57</span></span>
<span id="cb3-99"><a href="#cb3-99"></a></span>
<span id="cb3-100"><a href="#cb3-100"></a>    <span class="co">#   Target layout for x_gathered (shape: [58, h])</span></span>
<span id="cb3-101"><a href="#cb3-101"></a>    <span class="co">#  +------------------------------------------------+</span></span>
<span id="cb3-102"><a href="#cb3-102"></a>    <span class="co">#  |                                                |</span></span>
<span id="cb3-103"><a href="#cb3-103"></a>    <span class="co">#  |  All 36 tokens for MY Expert 0                 |</span></span>
<span id="cb3-104"><a href="#cb3-104"></a>    <span class="co">#  |  (Gathered from the 4 blocks above)            |</span></span>
<span id="cb3-105"><a href="#cb3-105"></a>    <span class="co">#  |                                                |</span></span>
<span id="cb3-106"><a href="#cb3-106"></a>    <span class="co">#  +------------------------------------------------+  &lt;-- Boundary at index 35</span></span>
<span id="cb3-107"><a href="#cb3-107"></a>    <span class="co">#  |                                                |</span></span>
<span id="cb3-108"><a href="#cb3-108"></a>    <span class="co">#  |  All 22 tokens for MY Expert 1                 |</span></span>
<span id="cb3-109"><a href="#cb3-109"></a>    <span class="co">#  |  (Gathered from the 4 blocks above)            |</span></span>
<span id="cb3-110"><a href="#cb3-110"></a>    <span class="co">#  |                                                |</span></span>
<span id="cb3-111"><a href="#cb3-111"></a>    <span class="co">#  +------------------------------------------------+ </span></span>
<span id="cb3-112"><a href="#cb3-112"></a></span>
<span id="cb3-113"><a href="#cb3-113"></a>    <span class="co"># target for num_tokens_per_expert_group</span></span>
<span id="cb3-114"><a href="#cb3-114"></a>    <span class="co">#    [36, 22]</span></span>
<span id="cb3-115"><a href="#cb3-115"></a></span>
<span id="cb3-116"><a href="#cb3-116"></a></span>
<span id="cb3-117"><a href="#cb3-117"></a>    <span class="co"># Reshape to see GPU-expert structure</span></span>
<span id="cb3-118"><a href="#cb3-118"></a>    tokens <span class="op">=</span> num_tokens_per_expert_group.view(<span class="op">-</span><span class="dv">1</span>, ep_size)  </span>
<span id="cb3-119"><a href="#cb3-119"></a>    <span class="co"># Shape: [4, 2] where dim0=GPU, dim1=expert</span></span>
<span id="cb3-120"><a href="#cb3-120"></a>    <span class="co"># [[10,  5],  &lt;- GPU 0: 10 tokens for E0, 5 for E1</span></span>
<span id="cb3-121"><a href="#cb3-121"></a>    <span class="co">#  [ 9,  4],  &lt;- GPU 1: 9 tokens for E0, 4 for E1</span></span>
<span id="cb3-122"><a href="#cb3-122"></a>    <span class="co">#  [14,  2],  &lt;- GPU 2: 14 tokens for E0, 2 for E1</span></span>
<span id="cb3-123"><a href="#cb3-123"></a>    <span class="co">#  [ 3, 11]]  &lt;- GPU 3: 3 tokens for E0, 11 for E1</span></span>
<span id="cb3-124"><a href="#cb3-124"></a>    expert_per_device <span class="op">=</span> num_tokens_per_expert_group.shape[<span class="dv">0</span>] <span class="op">//</span> ep_size</span>
<span id="cb3-125"><a href="#cb3-125"></a>    expert_ids <span class="op">=</span> torch.repeat_interleave(</span>
<span id="cb3-126"><a href="#cb3-126"></a>        torch.arange(expert_per_device).repeat(ep_size).to(<span class="st">'cuda'</span>),  <span class="co"># [0, 1, 0, 1, 0, 1, 0, 1] - expert pattern</span></span>
<span id="cb3-127"><a href="#cb3-127"></a>        num_tokens_per_expert_group  <span class="co"># [10,5,9,4,14,2,3,11] - repeat counts</span></span>
<span id="cb3-128"><a href="#cb3-128"></a>    )</span>
<span id="cb3-129"><a href="#cb3-129"></a>    </span>
<span id="cb3-130"><a href="#cb3-130"></a>    <span class="co"># index looks like</span></span>
<span id="cb3-131"><a href="#cb3-131"></a>    <span class="co"># tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 15, 16, 17, 18, 19, 20, 21, 22,</span></span>
<span id="cb3-132"><a href="#cb3-132"></a>    <span class="co"># 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 46,</span></span>
<span id="cb3-133"><a href="#cb3-133"></a>    <span class="co"># 10, 11, 12, 13, 14, 24, 25, 26, 27, 42, 43, 47, 48, 49, 50, 51, 52, 53,</span></span>
<span id="cb3-134"><a href="#cb3-134"></a>    <span class="co"># 54, 55, 56, 57])</span></span>
<span id="cb3-135"><a href="#cb3-135"></a>    <span class="va">self</span>.index <span class="op">=</span> torch.argsort(expert_ids, stable<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-136"><a href="#cb3-136"></a>    x_reorganized <span class="op">=</span> x_gathered[<span class="va">self</span>.index, :]</span>
<span id="cb3-137"><a href="#cb3-137"></a></span>
<span id="cb3-138"><a href="#cb3-138"></a>    <span class="co"># per expert aggregation</span></span>
<span id="cb3-139"><a href="#cb3-139"></a>    num_tokens_per_expert_group_agg <span class="op">=</span> tokens.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-140"><a href="#cb3-140"></a></span>
<span id="cb3-141"><a href="#cb3-141"></a>    <span class="cf">return</span> x_reorganized, num_tokens_per_expert_group_agg</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="the-three-boxes-in-the-forward" class="level2">
<h2 class="anchored" data-anchor-id="the-three-boxes-in-the-forward">The Three “Boxes” in the Forward</h2>
<ol type="1">
<li><p><strong>Non-experts</strong>: FSDP(8) across all ranks</p>
<pre><code>all_gather(8) → compute → reduce_scatter(8)</code></pre></li>
<li><p><strong>EP dispatch/return</strong>: row-local all-to-all</p>
<pre><code>Pairs: (0↔1), (2↔3), (4↔5), (6↔7). Size-exchange then A2A.</code></pre></li>
<li><p><strong>Experts</strong>: FSDP(4) inside each column</p>
<pre><code>all_gather(4) → grouped GEMM → reduce_scatter(4)</code></pre></li>
</ol>
<p>For visually initiated, here is how the placement looks:</p>
<p><a href="https://tinkerings.dev/static/moe.html">🎮 <strong>View Interactive 3D Visualization</strong></a> - See how data flows through the Non-MoE, MoE FSDP, and Token Routing layers.</p>
<hr>
</section>
<section id="minimal-mesh-wrapping" class="level2">
<h2 class="anchored" data-anchor-id="minimal-mesh-wrapping">Minimal Mesh &amp; Wrapping</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="co"># 0) EP: assign experts to columns + install dispatch/combine hooks</span></span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="kw">class</span> ExpertParallel(ParallelStyle):</span>
<span id="cb7-4"><a href="#cb7-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb7-5"><a href="#cb7-5"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-6"><a href="#cb7-6"></a>        <span class="va">self</span>.input_splits <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>        <span class="va">self</span>.output_splits <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-8"><a href="#cb7-8"></a></span>
<span id="cb7-9"><a href="#cb7-9"></a>    <span class="co"># performing all-to-all dispatch on the input</span></span>
<span id="cb7-10"><a href="#cb7-10"></a>    <span class="kw">def</span> _token_dispatch(<span class="va">self</span>, mod, inputs, device_mesh):</span>
<span id="cb7-11"><a href="#cb7-11"></a>        ...</span>
<span id="cb7-12"><a href="#cb7-12"></a></span>
<span id="cb7-13"><a href="#cb7-13"></a>    <span class="at">@staticmethod</span></span>
<span id="cb7-14"><a href="#cb7-14"></a>    <span class="kw">def</span> _partition_fn(name, mod, device_mesh):</span>
<span id="cb7-15"><a href="#cb7-15"></a>        <span class="co"># shard on the expert dimension</span></span>
<span id="cb7-16"><a href="#cb7-16"></a>        <span class="cf">for</span> name, param <span class="kw">in</span> mod.named_parameters(recurse<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb7-17"><a href="#cb7-17"></a>            dist_param <span class="op">=</span> nn.Parameter(distribute_tensor(param, device_mesh, [Shard(<span class="dv">0</span>)]))</span>
<span id="cb7-18"><a href="#cb7-18"></a>            mod.register_parameter(name, dist_param)</span>
<span id="cb7-19"><a href="#cb7-19"></a></span>
<span id="cb7-20"><a href="#cb7-20"></a>    <span class="co"># performing all-to-all combine on the output</span></span>
<span id="cb7-21"><a href="#cb7-21"></a>    <span class="kw">def</span> _token_combine(<span class="va">self</span>, mod, routed_output, device_mesh):</span>
<span id="cb7-22"><a href="#cb7-22"></a>        routed_output <span class="op">=</span> all_to_all_single_autograd(</span>
<span id="cb7-23"><a href="#cb7-23"></a>            routed_output,</span>
<span id="cb7-24"><a href="#cb7-24"></a>            <span class="va">self</span>.input_splits,</span>
<span id="cb7-25"><a href="#cb7-25"></a>            <span class="va">self</span>.output_splits,</span>
<span id="cb7-26"><a href="#cb7-26"></a>            device_mesh.get_group(),</span>
<span id="cb7-27"><a href="#cb7-27"></a>        )</span>
<span id="cb7-28"><a href="#cb7-28"></a>        <span class="cf">return</span> routed_output</span>
<span id="cb7-29"><a href="#cb7-29"></a></span>
<span id="cb7-30"><a href="#cb7-30"></a>    <span class="kw">def</span> _apply(<span class="va">self</span>, module: nn.Module, device_mesh: DeviceMesh) <span class="op">-&gt;</span> nn.Module:</span>
<span id="cb7-31"><a href="#cb7-31"></a>        <span class="cf">return</span> distribute_module(</span>
<span id="cb7-32"><a href="#cb7-32"></a>            module,</span>
<span id="cb7-33"><a href="#cb7-33"></a>            device_mesh,</span>
<span id="cb7-34"><a href="#cb7-34"></a>            partition_fn<span class="op">=</span>ExpertParallel._partition_fn,</span>
<span id="cb7-35"><a href="#cb7-35"></a>            input_fn<span class="op">=</span><span class="va">self</span>._token_dispatch,</span>
<span id="cb7-36"><a href="#cb7-36"></a>            output_fn<span class="op">=</span><span class="va">self</span>._token_combine,</span>
<span id="cb7-37"><a href="#cb7-37"></a>        )</span>
<span id="cb7-38"><a href="#cb7-38"></a></span>
<span id="cb7-39"><a href="#cb7-39"></a></span>
<span id="cb7-40"><a href="#cb7-40"></a>parallelize_module(</span>
<span id="cb7-41"><a href="#cb7-41"></a>    model, device_mesh<span class="op">=</span>col_mesh,</span>
<span id="cb7-42"><a href="#cb7-42"></a>    parallelize_plan<span class="op">=</span>{<span class="st">"layers.*.moe.experts"</span>: ExpertParallel()},</span>
<span id="cb7-43"><a href="#cb7-43"></a>)</span>
<span id="cb7-44"><a href="#cb7-44"></a></span>
<span id="cb7-45"><a href="#cb7-45"></a><span class="co"># 1) FSDP on experts (rows): shard column-local experts across rows</span></span>
<span id="cb7-46"><a href="#cb7-46"></a><span class="cf">for</span> tb <span class="kw">in</span> model.transformer_blocks:</span>
<span id="cb7-47"><a href="#cb7-47"></a>    fully_shard(tb.moe.experts, mesh<span class="op">=</span>row_mesh, reshard_after_forward<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-48"><a href="#cb7-48"></a></span>
<span id="cb7-49"><a href="#cb7-49"></a><span class="co"># 2) FSDP on each block (dp): shard non-MoE 8-way</span></span>
<span id="cb7-50"><a href="#cb7-50"></a><span class="cf">for</span> tb <span class="kw">in</span> model.transformer_blocks:</span>
<span id="cb7-51"><a href="#cb7-51"></a>    fully_shard(tb, mesh<span class="op">=</span>dp_mesh, reshard_after_forward<span class="op">=</span><span class="va">False</span>)   <span class="co"># experts already DTensors on rows</span></span>
<span id="cb7-52"><a href="#cb7-52"></a></span>
<span id="cb7-53"><a href="#cb7-53"></a><span class="co"># 3) FSDP on root (dp): embeddings / head / leftovers</span></span>
<span id="cb7-54"><a href="#cb7-54"></a>fully_shard(model, mesh<span class="op">=</span>dp_mesh, reshard_after_forward<span class="op">=</span><span class="va">True</span>)     <span class="co"># True/False per memory tradeoff</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
<section id="pseudo-forward" class="level2">
<h2 class="anchored" data-anchor-id="pseudo-forward">Pseudo-Forward</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">def</span> forward_on_rank(row, col, x_rc):</span>
<span id="cb8-2"><a href="#cb8-2"></a>    <span class="co"># A) Non-MoE on FSDP: AG(8) → compute → RS(8)</span></span>
<span id="cb8-3"><a href="#cb8-3"></a>    h <span class="op">=</span> non_moE_stack(x_rc)</span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a>    <span class="co"># Router: expert id per token (e.g., [2,2,5,1,6,...])</span></span>
<span id="cb8-6"><a href="#cb8-6"></a>    eids <span class="op">=</span> router(h)</span>
<span id="cb8-7"><a href="#cb8-7"></a></span>
<span id="cb8-8"><a href="#cb8-8"></a>    <span class="co"># B) EP dispatch within this row: send tokens to owner column</span></span>
<span id="cb8-9"><a href="#cb8-9"></a>    h_owned <span class="op">=</span> ep_a2a_dispatch(h, eids, group<span class="op">=</span>{(row,<span class="dv">0</span>),(row,<span class="dv">1</span>)})</span>
<span id="cb8-10"><a href="#cb8-10"></a></span>
<span id="cb8-11"><a href="#cb8-11"></a>    <span class="co"># C) Experts on owner column: FSDP(4) across rows of this column</span></span>
<span id="cb8-12"><a href="#cb8-12"></a>    y_local <span class="op">=</span> experts_matmul(h_owned)           <span class="co"># AG(4) → GEMM → RS(4)</span></span>
<span id="cb8-13"><a href="#cb8-13"></a></span>
<span id="cb8-14"><a href="#cb8-14"></a>    <span class="co"># D) EP combine within this row: inverse A2A back to source rank</span></span>
<span id="cb8-15"><a href="#cb8-15"></a>    y <span class="op">=</span> ep_a2a_combine(y_local, eids, group<span class="op">=</span>{(row,<span class="dv">0</span>),(row,<span class="dv">1</span>)})</span>
<span id="cb8-16"><a href="#cb8-16"></a></span>
<span id="cb8-17"><a href="#cb8-17"></a>    <span class="co"># E) Tail Non-MoE on dp: AG(8) → compute → RS(8)</span></span>
<span id="cb8-18"><a href="#cb8-18"></a>    out <span class="op">=</span> non_moE_tail(y)</span>
<span id="cb8-19"><a href="#cb8-19"></a>    <span class="cf">return</span> out</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Backward (intuition)</strong>: EP’s A2A autograd returns activation grads to sources; expert parameter grads are reduce-scattered across the row group (the 4 that shard that expert). Non-MoE grads reduce-scatter across all 8.</p>
<hr>
</section>
<section id="after-parallelization-expected-placements" class="level2">
<h2 class="anchored" data-anchor-id="after-parallelization-expected-placements">After Parallelization: Expected Placements</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">def</span> show_placements(model, keys<span class="op">=</span>(<span class="st">"moe.experts"</span>,<span class="st">"attention"</span>,<span class="st">"router"</span>,<span class="st">"embeddings"</span>,<span class="st">"norm"</span>,<span class="st">"output"</span>)):</span>
<span id="cb9-2"><a href="#cb9-2"></a>    <span class="cf">for</span> name, p <span class="kw">in</span> model.named_parameters():</span>
<span id="cb9-3"><a href="#cb9-3"></a>        <span class="cf">if</span> <span class="bu">any</span>(k <span class="kw">in</span> name <span class="cf">for</span> k <span class="kw">in</span> keys):</span>
<span id="cb9-4"><a href="#cb9-4"></a>            pl <span class="op">=</span> <span class="bu">getattr</span>(p, <span class="st">"placements"</span>, <span class="va">None</span>)</span>
<span id="cb9-5"><a href="#cb9-5"></a>            kind <span class="op">=</span> <span class="st">"DTensor"</span> <span class="cf">if</span> pl <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="st">"LOCAL"</span></span>
<span id="cb9-6"><a href="#cb9-6"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">:&lt;60}</span><span class="ss"> -&gt; </span><span class="sc">{</span>kind<span class="sc">:7}</span><span class="ss"> </span><span class="sc">{</span>pl<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Example lines you should see:</p>
<pre><code>tok_embeddings.weight                                   -&gt; DTensor (Shard(dim=0),)
layers.0.attention.wq.weight                            -&gt; DTensor (Shard(dim=0),)
layers.0.feed_forward.router.router.weight              -&gt; DTensor (Shard(dim=0),)
layers.0.feed_forward.experts.w1                        -&gt; DTensor (_StridedShard(dim=0, sf=2), Shard(dim=0))
layers.0.feed_forward.experts.w2                        -&gt; DTensor (_StridedShard(dim=0, sf=2), Shard(dim=0))
layers.0.feed_forward.experts.w3                        -&gt; DTensor (_StridedShard(dim=0, sf=2), Shard(dim=0))
...</code></pre>
<section id="legend" class="level3">
<h3 class="anchored" data-anchor-id="legend">Legend</h3>
<ul>
<li><code>(Shard(dim=0),)</code> → Non-MoE params sharded on a 1-D mesh (flattened dp → 8-way).</li>
<li><code>(_StridedShard(dim=0, sf=2), Shard(dim=0))</code> → Experts are split on <strong>two separete mesh axes</strong> of the same tensor dim-0 (experts):
<ul>
<li><code>Shard(dim=0)</code> on cols (EP ownership).</li>
<li><code>_StridedShard(dim=0, sf=2)</code> on rows (FSDP across 4 rows, after a prior split by 2 cols). <code>sf=2</code> because dim-0 was already split once by the 2 columns</li>
</ul></li>
</ul>
</section>
</section>
<section id="appendix-dataloader-on-flattened-dp." class="level2">
<h2 class="anchored" data-anchor-id="appendix-dataloader-on-flattened-dp.">Appendix: Dataloader on Flattened DP.</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># Use the flattened 8-way dp mesh for sampling</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>global_dp_rank <span class="op">=</span> dp_mesh.get_rank()</span>
<span id="cb11-3"><a href="#cb11-3"></a>num_replicas   <span class="op">=</span> dp_mesh.size()</span>
<span id="cb11-4"><a href="#cb11-4"></a></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DistributedSampler, DataLoader</span>
<span id="cb11-6"><a href="#cb11-6"></a>sampler <span class="op">=</span> DistributedSampler(dataset, num_replicas<span class="op">=</span>num_replicas, rank<span class="op">=</span>global_dp_rank)</span>
<span id="cb11-7"><a href="#cb11-7"></a>loader  <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span>per_rank_bsz, sampler<span class="op">=</span>sampler, pin_memory<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This preserves 8 microbatches per step—one per rank—while placements and collectives follow the mesh rules above.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/tinkerings\.dev");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>
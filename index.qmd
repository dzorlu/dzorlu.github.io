---
title: "Expert Parallel"
date: "2025-09-01"
categories: [distributed-training]
page-layout: article
---

Let's distill how to run a Mixture-of-Experts (MoE) model on 8 GPUs using a 2-D device mesh with Expert Parallel (EP) and FSDP2 (fully_shard). It explains who stores which weights, where all-to-all happens, why we choose EP=2, and includes minimal code (meshes, wrapping, verification).

---

## TL;DR (mental picture)
- **Columns move tokens. Rows share weights.**
- **Cols** (dp_shard_in_ep) = EP axis → which experts live where + token all-to-all.
- **Rows** (dp_shard_mod_ep) = FSDP axis → how owned experts are sharded.
- **Non-MoE**: FSDP(8) across all ranks (no persistent replication).
- **Experts**: EP(2) across columns (ownership) × FSDP(4) across rows (shards).
- **EP all-to-all** is row-local pairs: (0↔1), (2↔3), (4↔5), (6↔7). Microbatches can differ (variable-size A2A handles it).

---

## Device Mesh & Rank Layout

Arrange 8 GPUs as a 4×2 grid:

```
(rows, cols) → rank
(0,0)→0   (0,1)→1
(1,0)→2   (1,1)→3
(2,0)→4   (2,1)→5
(3,0)→6   (3,1)→7
```

- **Rows** (dp_shard_mod_ep) size 4 → FSDP sharding axis for experts.
- **Cols** (dp_shard_in_ep) size 2 → EP ownership + row-local all-to-all.
- **Flattened dp** view (rows×cols) size 8 → dataloader + FSDP for non-MoE.

---

## Who Stores Which Expert?

Assume 8 experts E0..E7. With EP=2:
- **Column 0** owns experts E0–E3 (no copy of E4–E7).
- **Column 1** owns experts E4–E7 (no copy of E0–E3).
- Inside each column, FSDP(4) shards the owned experts across the 4 rows.

| Rank | Column owns | This rank holds (expert weights) |
|------|-------------|----------------------------------|
| 0 = (0,0) | E0–E3 | 1/4 shard of E0–E3 |
| 2 = (1,0) | E0–E3 | 1/4 shard of E0–E3 |
| 4 = (2,0) | E0–E3 | 1/4 shard of E0–E3 |
| 6 = (3,0) | E0–E3 | 1/4 shard of E0–E3 |
| 1 = (0,1) | E4–E7 | 1/4 shard of E4–E7 |
| 3 = (1,1) | E4–E7 | 1/4 shard of E4–E7 |
| 5 = (2,1) | E4–E7 | 1/4 shard of E4–E7 |
| 7 = (3,1) | E4–E7 | 1/4 shard of E4–E7 |

Non-MoE (embeddings, attention, MLP, norms) are FSDP-sharded 8-way across all ranks (flattened dp). There is no persistent replication.

---

## Where Does All-to-All Happen?

EP all-to-all is **row-local**:
- Row 0: GPU0 ⇄ GPU1
- Row 1: GPU2 ⇄ GPU3
- Row 2: GPU4 ⇄ GPU5
- Row 3: GPU6 ⇄ GPU7

Each rank runs the router on its own microbatch. Tokens are split by destination column (owner of the chosen expert), then a size-exchange + variable-size all-to-all exchanges exactly those slices between the two columns in the same row. Different microbatches are fine.

After dispatch, each rank holds only tokens for experts it owns. Expert matmuls then use FSDP across the 4 rows in that column (all-gather → compute → reduce-scatter). Finally, the inverse all-to-all returns outputs to the source ranks.

---

## The Three "Boxes" in the Forward

1. **Non-experts**: FSDP(8) across all ranks
   ```
   all_gather(8) → compute → reduce_scatter(8)
   ```

2. **EP dispatch/return**: row-local all-to-all
   ```
   Pairs: (0↔1), (2↔3), (4↔5), (6↔7). Size-exchange then A2A.
   ```

3. **Experts**: FSDP(4) inside each column
   ```
   all_gather(4) → grouped GEMM → reduce_scatter(4)
   ```

---

## Minimal Mesh & Wrapping (PR-faithful order)

```python
from torch.distributed.tensor import init_device_mesh
from torch.distributed.tensor.parallel import parallelize_module
from torch.distributed.fsdp import fully_shard

# 2-D mesh: rows (FSDP for experts), cols (EP ownership & A2A)
world = init_device_mesh("cuda", (4, 2), mesh_dim_names=("rows","cols"))
world[("rows","cols")]._flatten(mesh_dim_name="dp")   # dataloader + FSDP(non-MoE)

row_mesh = world["rows"]   # size 4
col_mesh = world["cols"]   # size 2
dp_mesh  = world["dp"]     # size 8

# 0) EP: assign experts to columns + install dispatch/combine hooks
parallelize_module(
    model, device_mesh=col_mesh,
    parallelize_plan={"layers.*.moe.experts": ExpertParallel()},
)

# 1) FSDP on experts (rows): shard column-local experts across rows
for tb in model.transformer_blocks:
    if getattr(tb, "moe_enabled", False):
        fully_shard(tb.moe.experts, mesh=row_mesh, reshard_after_forward=False)

# 2) FSDP on each block (dp): shard non-MoE 8-way
for tb in model.transformer_blocks:
    fully_shard(tb, mesh=dp_mesh, reshard_after_forward=False)   # experts already DTensors on rows

# 3) FSDP on root (dp): embeddings / head / leftovers
fully_shard(model, mesh=dp_mesh, reshard_after_forward=True)     # True/False per memory tradeoff
```

---

## Pseudo-Forward (annotated)

```python
def forward_on_rank(row, col, x_rc):
    # A) Non-MoE on dp: AG(8) → compute → RS(8)
    h = non_moE_stack(x_rc)

    # Router: expert id per token (e.g., [2,2,5,1,6,...])
    eids = router(h)

    # B) EP dispatch within this row: send tokens to owner column
    h_owned = ep_a2a_dispatch(h, eids, group={(row,0),(row,1)})

    # C) Experts on owner column: FSDP(4) across rows of this column
    y_local = experts_matmul(h_owned)           # AG(4) → GEMM → RS(4)

    # D) EP combine within this row: inverse A2A back to source rank
    y = ep_a2a_combine(y_local, eids, group={(row,0),(row,1)})

    # E) Tail Non-MoE on dp: AG(8) → compute → RS(8)
    out = non_moE_tail(y)
    return out
```

**Backward (intuition)**: EP's A2A autograd returns activation grads to sources; expert parameter grads are reduce-scattered across the row group (the 4 that shard that expert). Non-MoE grads reduce-scatter across all 8.

---

## After Parallelization: Expected Placements (and how to print them)

### Legend
- `(Shard(dim=0),)` → Non-MoE params sharded on a 1-D mesh (flattened dp → 8-way).
- `(_StridedShard(dim=0, sf=2), Shard(dim=0))` → Experts are split on two mesh axes of the same tensor dim-0 (experts):
  - `Shard(dim=0)` on cols (EP ownership).
  - `_StridedShard(dim=0, sf=2)` on rows (FSDP across 4 rows, after a prior split by 2 cols).
  
No replication — this is a 2-D partition of expert dim-0.

```python
def show_placements(model, keys=("moe.experts","attention","router","embeddings","norm","output")):
    for name, p in model.named_parameters():
        if any(k in name for k in keys):
            pl = getattr(p, "placements", None)
            kind = "DTensor" if pl is not None else "LOCAL"
            print(f"{name:<60} -> {kind:7} {pl}")
```

Example lines you should see (shape may vary; this is pattern):

```
tok_embeddings.weight                                   -> DTensor (Shard(dim=0),)
layers.0.attention.wq.weight                            -> DTensor (Shard(dim=0),)
layers.0.feed_forward.router.router.weight              -> DTensor (Shard(dim=0),)
layers.0.feed_forward.experts.w1                        -> DTensor (_StridedShard(dim=0, sf=2), Shard(dim=0))
layers.0.feed_forward.experts.w2                        -> DTensor (_StridedShard(dim=0, sf=2), Shard(dim=0))
layers.0.feed_forward.experts.w3                        -> DTensor (_StridedShard(dim=0, sf=2), Shard(dim=0))
...
```

### Mini visual for `_StridedShard(dim=0, sf=2)`

```
experts dim-0:  [ 0 1 2 3 | 4 5 6 7 ]
                 ^^^^^^^^   ^^^^^^^^
                 col 0       col 1     (EP: Shard dim-0 by columns)

within each column (rows=4):
col 0 → row shards of [0 1 2 3]   → rows 0..3 each get 1 id
col 1 → row shards of [4 5 6 7]   → rows 0..3 each get 1 id

DTensor encodes "shard dim-0 again on rows" as _StridedShard(dim=0, sf=2)
(sf=2 because dim-0 was already split once by the 2 columns).
```

---

## Why EP=2 and not EP=8?

- **EP=2 + rows=4** (this README):
  - **Memory**: experts are also sharded (rows).
  - **Comms**: A2A is 2-way (cheap).
  - **Compute**: healthy per-expert batches per device → efficient grouped GEMM.

- **EP=8 + rows=1**:
  - No rows left → cannot FSDP-shard expert weights (higher memory).
  - A2A is 8-way (heavier comms) and per-expert batches per GPU get tiny (worse efficiency).
  - You can still FSDP(8) non-MoE, but overall MoE trade-offs are usually worse.

---

## Sanity Checklist

- Non-MoE params print one placement → `(Shard(dim=0),)` (FSDP-8 on dp).
- Expert params print two placements → `(_StridedShard(0,sf=2), Shard(0))` (rows×cols).
- No `Replicate()` in placements → no persistent replication.
- EP A2A pairs (0↔1), (2↔3), (4↔5), (6↔7) — microbatches can differ (size-exchange + variable-size A2A).

---

## Appendix: Dataloader on Flattened dp

```python
# Use the flattened 8-way dp mesh for sampling
global_dp_rank = dp_mesh.get_rank()
num_replicas   = dp_mesh.size()

from torch.utils.data import DistributedSampler, DataLoader
sampler = DistributedSampler(dataset, num_replicas=num_replicas, rank=global_dp_rank)
loader  = DataLoader(dataset, batch_size=per_rank_bs, sampler=sampler, pin_memory=True)
```

This preserves 8 microbatches per step—one per rank—while placements and collectives follow the mesh rules above.
